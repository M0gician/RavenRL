{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will construct a seldonian version of *Cross Entropy Method* learning algorithm and train over a classic RL environemnt -- Cart Pole.\r\n",
    "If you are not familiar with the concept and the definition of Seldonian framework, you can read the supplementary paper [here](https://science.sciencemag.org/content/366/6468/999)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we need to import all the libraries and dependencies\r\n",
    "\r\n",
    "# pickle for saving the experiment result (if needed)\r\n",
    "import pickle\r\n",
    "\r\n",
    "# RavenRL.Env contains predefined RL environemnts (including Cart Pole)\r\n",
    "from RavenRL.Env import setup_env\r\n",
    "\r\n",
    "# RavenRL.Utils contains auxiliary helper functions \r\n",
    "from RavenRL.Utils import generate_dataset, safety_test\r\n",
    "\r\n",
    "# RavenRL.Policy contains definitions of RL Policies\r\n",
    "from RavenRL.Policy import RandomPolicy, DiscretePolicy\r\n",
    "\r\n",
    "# RavenRL.Agent contains definitions of RL learning algorithms\r\n",
    "from RavenRL.Agent import CEMSeldonian, g0\r\n",
    "\r\n",
    "# RavenRL.Sampling contains definitions of different Importance Sampling Methods\r\n",
    "from RavenRL.Sampling import PDIS\r\n",
    "\r\n",
    "# RavenRL.Func contains definitions of concentration bounds\r\n",
    "from RavenRL.Func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we define our environment name and other hyperparameters\r\n",
    "\r\n",
    "env_id = \"cartpole\"\r\n",
    "n_proc = 8\r\n",
    "train_size = 5000\r\n",
    "test_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we initialize the test environment and use it to make dataset for later experiment\r\n",
    "\r\n",
    "env, _, _ = setup_env(env_id)\r\n",
    "dataset_train = generate_dataset(env_id, train_size, n_proc=n_proc)\r\n",
    "dataset_test = generate_dataset(env_id, test_size, n_proc=n_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, we initialize a behavioral policy for Importance Sampling Estimator\r\n",
    "\r\n",
    "behv_policy = RandomPolicy(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third, we initialize two Per-Decision Importance Sampling Estimators for training and testing dataset\r\n",
    "\r\n",
    "sampler_train = PDIS(dataset_train, behv_policy, gamma=1, n_proc=n_proc)\r\n",
    "sampler_test = PDIS(dataset_test, behv_policy, gamma=1, n_proc=n_proc)\r\n",
    "\r\n",
    "# For concentration bound, we will use a new bound recently published by Erik Learned-Miller and Philip S. Thomas (https://arxiv.org/abs/1905.06208)\r\n",
    "ci = mcma_ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have all the parts we needed. Here we can initialized our Seldonian CEM learning method and start training\r\n",
    "\r\n",
    "\r\n",
    "g_funcs = [g0]  # The predefined performance constraint function ( J(pi) > 20 )\r\n",
    "\r\n",
    "n_sample = 50   # The number of trajectories that will be tested on each policy which generated by CEM\r\n",
    "\r\n",
    "# Initialize a Seldonian CEM \r\n",
    "agent = CEMSeldonian(\r\n",
    "    epochs=10, pop_size=30, elite_ratio=0.17, n_sample=n_sample,\r\n",
    "    ci_ub=ci, ref_size=5000, g_funcs=g_funcs, correction=1,\r\n",
    "    gamma=1, extra_std=2.0, extra_decay_time=10, n_proc=n_proc\r\n",
    ")\r\n",
    "\r\n",
    "agent.load_env(env_id)              # Load environemnt info\r\n",
    "agent.load_sampler(sampler_train)   # Load training PDIS estimator\r\n",
    "agent.init_params()                 # Initialize parameters\r\n",
    "agent.train()                       # Train!\r\n",
    "\r\n",
    "# This training process takes about 2-3 minutes. Please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, we can get our candidate policies out from the CEM\r\n",
    "\r\n",
    "thetas = agent.get_best_candidates()\r\n",
    "if len(thetas.shape) == 1:\r\n",
    "    thetas = thetas.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we perform a safety test using a new testing data to ensure all returned candidate policies \r\n",
    "# have a performance lower bound higher than the performance constraint with a high confidence\r\n",
    "\r\n",
    "solutions = []\r\n",
    "for theta in thetas:\r\n",
    "    sol = safety_test(\r\n",
    "        env_id=env_id, theta=theta, sampler=sampler_test, ref_size=test_size,\r\n",
    "        ci_ub=ci, g_funcs=g_funcs, delta=0.05/thetas.shape[0]\r\n",
    "    )\r\n",
    "    solutions.append(sol)\r\n",
    "\r\n",
    "print(solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will run the returned safe policies over the environemnt directly to check if they are relly \"safe\" as we expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to filter out policies which didn't pass the safety test\r\n",
    "# and we can also calculate the ratio of how many candidate policies finally pass the safety test. \r\n",
    "\r\n",
    "elites = []\r\n",
    "for sol in solutions:\r\n",
    "    if not isinstance(sol[1], str):\r\n",
    "        elites.append(sol[1])\r\n",
    "yield_ratio = len(elites) / len(solutions)\r\n",
    "print(f\"yield ratio: {yield_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we test each \"safe policy\" over the environment directly for 30 times and collect all of their real performance\r\n",
    "\r\n",
    "n_trials = 30\r\n",
    "rewards = []\r\n",
    "for theta in elites:\r\n",
    "    theta_r = []\r\n",
    "    for _ in range(n_trials):\r\n",
    "        policy = DiscretePolicy(env, theta=theta)\r\n",
    "        R = 0\r\n",
    "        done = False\r\n",
    "        s = env.reset()\r\n",
    "        while not done:\r\n",
    "            a = policy.act(s)\r\n",
    "            s_prime, r, done, _ = env.step(a)\r\n",
    "            R += r\r\n",
    "            s = s_prime\r\n",
    "        theta_r.append(R)\r\n",
    "    theta_r = np.array(theta_r)\r\n",
    "    rewards.append(theta_r)\r\n",
    "rewards = np.array(rewards).ravel()\r\n",
    "print(f\"rewards: {rewards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can calculate the violation rate of all \"safe policies\" we get from the safety test\r\n",
    "# Since we are using a significance level of 0.05, we are expecting a violation ratio < 0.05\r\n",
    "\r\n",
    "perf_lb = 20\r\n",
    "violations = rewards[rewards<perf_lb]\r\n",
    "violation_prob = 0\r\n",
    "if violations.size != 0:\r\n",
    "    violation_prob = violations.size / rewards.size\r\n",
    "if len(rewards) > 0:\r\n",
    "    print(f\"max: {rewards.max()}\")\r\n",
    "    print(f\"min: {rewards.min()}\")\r\n",
    "    print(f\"avg: {np.average(rewards)}\")\r\n",
    "    print(f\"violation prob: {violation_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pickle to save the experiment results if needed\r\n",
    "\r\n",
    "with open(f'cartpole_{n_sample}', 'wb') as f:\r\n",
    "    obj = [yield_ratio, rewards, violation_prob]\r\n",
    "    pickle.dump(obj, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}